---
title: "HW4_1"
author: "Lizhao"
date: '2022-04-22'
output: 
  md_document
---

### Clustering and PCA
```{r setup, include=FALSE}
wine <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv')
library(tidyverse)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)
library(knitr)

```

#### Clustering model
we first center and scale the data
```{r center and scale data, include=FALSE}
X = wine[,(1:11)]
X = scale(X, center=TRUE, scale=TRUE)

mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
distance_between_wines = dist(X)
```

##### distinguishing colors
To distinguish color, we first cluster the dataset into two parts, then we run k-means with 2 clusters and 25 starts, and I create plots to show different clusters' color and other variables.
```{r cluster, include=FALSE}
clust1 = kmeans(X, 2, nstart=25)

```

Since there are many variables, we only choose pH value, density, sulphates as x-axis variable, and see whether the cluster model can distinguish their colors.

```{r plot for pH & density, echo=FALSE}
qplot(pH, color, data=wine,color=factor(clust1$cluster))
qplot(alcohol, color, data=wine,color=factor(clust1$cluster))
qplot(sulphates, color, data=wine,color=factor(clust1$cluster))
```
we can clearly see that cluster 1 is almost white and cluster 2 is almost red, no matter what variable is on the x-axis.

then we use kmeans++ method, and get its normalized data and plots. (we still use __pH__, __alcohol__, __sulphates__)


```{r kmeans++ initialization, echo=FALSE}
clust2 = kmeanspp(X, k=2, nstart=25)
clust2$center[1,]*sigma + mu

```

```{r kpp plots, echo=FALSE}
qplot(pH, color, data=wine,color=factor(clust2$cluster))
qplot(alcohol, color, data=wine,color=factor(clust2$cluster))
qplot(sulphates, color, data=wine,color=factor(clust2$cluster))
```
we also try hierarchical clustering, but the size of cluster is not very balanced, so we don't include too much details for this methods. I just show 6 clusters size under this method.

```{r hierarchical clustering, echo=FALSE}
X = wine[,(1:11)]
X = scale(X, center=TRUE, scale=TRUE)

mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
distance_between_wines = dist(X, method='euclidean')
hier_wine = hclust(distance_between_wines, method='average')
cluster3 = cutree(hier_wine, k=6)
summary(factor(cluster3))
```


##### distinguish quality
In this case, I set 6 clusters and we create plots to show that different clusters with different qualites. We use __pH__, __alcohol__, __sulphates__ as x-axis separately, and try to find any features about their qualities.

```{r make 6 clusters, include=FALSE}
clust3 = kmeans(X, 6, nstart=25)
```

```{r cluster3 plots, echo=FALSE}

qplot(pH, quality, data=wine, color=factor(clust3$cluster))
qplot(alcohol, quality, data=wine, color=factor(clust3$cluster))
qplot(sulphates, quality, data=wine, color=factor(clust3$cluster))


```


from the table we can see that almost each quality level contain different clusters, it is not as obvious as we can distinguish the color previously. On the other hand, we can see that the size of cluster affect its distribution, since larger size, larger distribution.



#### PCA model
Then we use the same variables as used in clustering models, and try to do some unsupervised calculation.

##### Estimating color
To estimate color, we use PCA to observe each PCA variable's loading, in order to see whether the differences in the labels (red/white) emerge naturally from applying an unsupervised technique to the chemical properties. 

Firstly, due to we have 11 initial chemical indexes, I set 3 PCA variables to describe initial indexes. 

```{r set red wine table, include=FALSE}
Z = wine[, (1:11)]
PCA_wine = prcomp(Z, scale = TRUE, rank = 3)
```

and we can use the PCA model we got to estimated value for initial each sample. Then we create a table and add their corresponding color and quality.

```{r use PCA to predict result, include=FALSE}
pred_color = predict(PCA_wine,Z)
pred_color =  as.data.frame(pred_color)
pred_color$color = wine$color
pred_color$quality = wine$quality
```

```{r shows the PCA estimated value tables, echo=FALSE}
kable(pred_color, caption = "PCA estimated value table")
```

Then we can create plots to show that for different color wines, their estimated PCA variable value is different.

```{r show the pca plots, echo=FALSE}

p1 = ggplot(data = pred_color) + geom_point(aes(x = PC1, y = color), alpha = 0.02) + labs(title = 'PCA1 for different colors ')
p2 = ggplot(data = pred_color) + geom_point(aes(x = PC2, y = color), alpha = 0.02) + labs(title = 'PCA2 for different colors')
p3 = ggplot(data = pred_color) + geom_point(aes(x = PC3, y = color), alpha = 0.02) + labs(title = 'PCA3 for different colors')
p1
p2
p3
```

According to these plots, we can see that for variable `PC1`, we can see estimated values indeed has different distribution for different colors, white wine's estimated value is almost around __(-1.5, 2.5)__ , red wine estimated value is almost around __(-4, -0.5)__. White wine has a wider distribution than red wine in terms of estimated PC2's values. Red wine has a wider distribution than white wine in terms of estimated PC3's values.


Then we continue to plot different PCA variables value for different quality.

```{r PCA variables for quality, echo=FALSE}
p1 = ggplot(data = pred_color) + geom_point(aes(x = PC1, y = quality), alpha = 0.02) + labs(title = 'PCA1 for differnt qualities ')

p2 = ggplot(data = pred_color) + geom_point(aes(x = PC2, y = quality), alpha = 0.02) + labs(title = 'PCA2 for differnt qualities ')

p3 = ggplot(data = pred_color) + geom_point(aes(x = PC3, y = quality), alpha = 0.02) + labs(title = 'PCA3 for differnt qualities ')

p1
p2
p3

```

Due to different sizes for different quality levels, we cannot distinguish a obvious features for any specific quality levels in terms of PCA1 estimated variables. However, in terms of PCA2 variables' distribution, quality level 6 has a wider distribution than other levels, same thing happen in terms of PCA3 estimated values.

Then let's see the loading summary table for the PCA model. 


```{r loading summary tables, echo=FALSE, results='asis'}
loadings_summary = PCA_wine$rotation %>%
  as.data.frame() %>%
  rownames_to_column('index')



kable(loadings_summary[,1:3], caption = "PCA loading summary table")


```

From these two tables, we can indeed see that different color wine's indexes has different feature in terms of estimated PCA variables. 

Also, we cannot distinguish different quality levels according to PCA's predictions based on above plots, since their estimated variable's distributions are similar.

###### interesting findings

From the loading summary table, we can see that different quality levels has different negative loading for various variables. For example, in terms of PCA1, the largest negative loading is `volatile.acidity`, about `-0.38`, but its loading would be always positive in terms of PCA2, and PCA3. On the other hand, the largest positive loading is `total.sulfur.dioxide` in terms of PCA1, around `0.49`, but for other two PCA variables, there are both very small.

### summary
In general, clustering model and PCA model can distinguish wines from their colors, and we can see the result according to plots. But for distinguishing quality, both of these two models' performance don't have enough plausible result. 













