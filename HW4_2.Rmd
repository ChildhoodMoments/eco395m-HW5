---
title: "HW4_2"
author: "Lizhao"
date: '2022-04-25'
output: md_document
---

```{r setup, include=FALSE}
NutrientH20 <- read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
library(tidyverse)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)
library(knitr)
```
## Introduction

This data set *NutrientH20* contains each user's Twitter post. To analyse interesting market segments that appear to stand out in their social-media audience, I just use two different methods to estiamte what knid of relatioship between these followers: _1. cluster model_, _2. network model_, since there is some inevitable error and noisiness in the annotation process, we use __average distance__ when setting clusters. Next, we want to see for the largest cluster, is there any feature that the NutrientH20 can utilize, in order to target consumers better


### method 1: cluster model

I'll start by grouping users with the same twitter post tag to see what happens.

```{r kmeans, include=FALSE}

X = NutrientH20[,(2:37)]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
```


we first use k-means with 10 clusters and 50 starts, then we will use kmeans++ initialization, then finally we will use hierarchical clustering method, in order to see is there any difference between these cluster methods.
##### k-means
```{r k-means , include=FALSE}
clust1 = kmeans(X, 10, nstart=50)

```

Firstly, we divide them into 10 clusters. Then we can show the size and mean value for each tag in terms of different clusters. 

```{r clust1 shows, echo=FALSE}
set.seed(1000)
clust1$size
clust1_sum <- clust1$center%>% as.data.frame()%>%rownames_to_column('cluster_No')
clust1_sum
```
we can see that cluster 3 has the largest size. To do analysis, we select top 3 clusters (cluster 3, 10, 1), we select the top three largest clusters to analyze their characteristics, so as to maximize the positioning of their potential consumers.

```{r biggest cluster, echo=FALSE}
biggest_clust <-  clust1_sum[clust1_sum$cluster_No == 3 | clust1_sum$cluster_No == 10 | clust1_sum$cluster_No == 1, ]
target <- c(3,10,1)
biggest_clust <- biggest_clust[match(target, biggest_clust$cluster_No), ]
# https://stackoverflow.com/questions/11977102/order-data-frame-rows-according-to-vector-with-specific-order

kable(biggest_clust, caption = 'Top 3 biggest clusters data under kmeans')
```

From the table we can see that, under normalized data, cluster 3 has larger number of twitters about we can estimate that for largest followers group, they tend to have lower twitters than other group. It is a weird conclusion, but according to the table, they indeed has negative value for each tags. There are serval possibility: 1. they like using twitter and post some topics that not be included in given tags.  2. It is totally possible that they seldom post twitters since the value of **uncategorized** tag is also negative, as we always said: __the silent majority__.

On the other hand, clusetr 10 has a obvious larger value in terms of **chatter** and **photo-sharing** and **shopping** tags(larger than 1), also have a relative larger number of twitters about  **business** and **current_events**. Due to its lager size (1065), we can conclude that NutrientH20 should focus more on promoting products in these areas in order to better attract followers to buy products. 

##### k++ means 
we then use `k++ means` method to do clustering again. 

```{r k means ++clusters, include=FALSE}
clust2 = kmeanspp(X, k=10, nstart=50)
```

Then we can show the size and mean value for each tag in terms of different clusters
```{r  k++ sum, echo=FALSE}
set.seed(10000)
clust2$size
clust2_sum <- clust2$center%>% as.data.frame()%>%rownames_to_column('cluster_No')
clust2_sum
```
we can see that cluster 8 has the largest size. As before, we extract the data for the first three largest groups.
```{r k++ summary, echo=FALSE}
biggest_clust2 <- clust2_sum[clust2_sum$cluster_No ==8 | clust2_sum$cluster_No == 5 | clust2_sum$cluster_No ==9, ]

target <- c(8,5,9)
biggest_clust2 <- biggest_clust2[match(target, biggest_clust2$cluster_No), ]

kable(biggest_clust2, caption = 'Top 3 biggest clusters data under kmeans++')
```
we can get almost same result as we got from previous kmeans method. So I will not repeat it again.



##### hierarchical clustering
we try to use hierarchical cluster model, and analyse the features of some big clusters.

```{r hierarchical clustering, include=FALSE}
X = NutrientH20[,(2:37)]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

# First form a pairwise distance matrix
distance_matrix = dist(X)

# Now run hierarchical clustering
h1 = hclust(distance_matrix, method='complete')
clust3 = cutree(h1, k=10, )
D = data.frame(X, z = clust3)

```

```{r show hierarchical clustering, echo=FALSE}
summary(factor(clust3))
D_2 = D %>% group_by(z)%>%summarise_all('mean')
#https://stackoverflow.com/questions/40947288/how-to-calculate-mean-of-all-columns-by-group
```

we can see that cluster 2 has the largest size. So we focus on this biggest cluster. So we still use the conclusion from __kmeans__ and __kmeans++__. 


```{r show largest cluster, echo=FALSE}
D_3 <- D_2[D_2$z ==2, ] %>% as.data.frame()
# https://stackoverflow.com/questions/9729962/extracting-rows-based-on-the-value
D_3
```
According to the result, we can see that compare to other group, this group 's twitter has a slightly larger part about *chatter*, *beauty*, *fashion*, *photosharing*  tags, so NutrientH20should focus on combing their product promotion with these tags. On the other hand, we can see that their twitters contain less information about *travel*, *politics*, *news*, *online_gaming*, which means the NutrientH20 should spend less budget into advertisement through these social media, like travel magazine, newspaper, etc. Since their followers don't spend more attention to these topic.


##### PCA model:

We will use PCA model to reduce the initial tags into 4 PCA variables. Then we use PCA model to estimate each followers. 


```{r set PCA matrix, include=FALSE}
Z = NutrientH20[,(2:37)]
PCA_tag = prcomp(Z, scale = TRUE, rank = 4)

```

```{r use PCA to predict result, include=FALSE}
pred_tag <- predict(PCA_tag, Z)
pred_tag = pred_tag%>% as.data.frame()
```

Then we analyze the loadings summary for each tags. We use plots to show that different tags loadings in terms of different PCA variables. Be careful the __0-line__.

```{r show loading_summary, include=FALSE}
loadings_summary = PCA_tag$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Tag')

kable(loadings_summary[,1:5], caption = "PCA loading summary table")
```

```{r loading_summary plots, include=FALSE}
p1 = ggplot(data = loadings_summary, aes(x = Tag, y = PC1)) +
  geom_bar(stat = "identity")+ geom_hline(yintercept = 0.0, color = 'red', size = 1.0) + coord_flip() 

p2 = ggplot(data = loadings_summary, aes(x = Tag, y = PC2)) +
  geom_bar(stat = "identity")+ geom_hline(yintercept = 0.0, color = 'red', size = 1.0) + coord_flip()

p3 = ggplot(data = loadings_summary, aes(x = Tag, y = PC3)) +
  geom_bar(stat = "identity")+ geom_hline(yintercept = 0.0, color = 'red', size = 1.0) + coord_flip()

p1
p2
p3

```

We can see that most tags have negative loadings in terms of PCA1, *photo_sharing*, *fashion*, *cooking* contribute largest 3 positive loadings in terms of PCA2, and *personal_fitness*, *health_nutrition*, and *cooking* are 3 largest positive tags in terms of PCA3, so in general, NutrientH20 should focus on these tags according to their levels.  


Overall, we can conclude that cluster model and PCA model give us similar result according to their conclusions, and NutrientH20 could use these info to position their brand to maximally appeal to each market segment.














